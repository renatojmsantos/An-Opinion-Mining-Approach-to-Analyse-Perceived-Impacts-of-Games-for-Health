{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Install Packages","metadata":{"id":"Zy0EZN-kkJ5z"}},{"cell_type":"code","source":"!pip install pandas\n!pip install numpy\n!pip install emoji\n!pip install nltk\n!pip install emoji\n!pip install re\n!pip install textblob\n!pip install unicodedata\n!pip install sklearn\n!pip install imblearn\n","metadata":{"id":"rbMFefQtkGiu","outputId":"e7a044f4-8ea9-4554-8b8a-f3e2f1890b56","execution":{"iopub.status.busy":"2022-03-07T22:37:26.637356Z","iopub.execute_input":"2022-03-07T22:37:26.637651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Import dataset","metadata":{"id":"B1M1mQfA-ucI"}},{"cell_type":"code","source":"import numpy as np \nimport pandas as pd \n\n#dataset with 10k comments\ndf = pd.read_csv('../input/just-dance-on-youtube/jd-dataset-topics-Human-Review.csv')\n\ndf.drop('Unnamed: 0', axis=1, inplace=True)\ndf.drop('commentID', axis=1, inplace=True)\ndf.drop('expandedText', axis=1, inplace=True)\ndf.drop('Sexual activity', axis=1, inplace=True)\n\nprint(df.shape)\n\ndata = df\ndata.head()","metadata":{"id":"8Tuy3J6HLp-s","outputId":"bb0bd188-0365-4110-a421-166159e2fa40","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Define the topics to classify","metadata":{"id":"UqwBHmlt-oNP"}},{"cell_type":"code","source":"# define the 37 target topics of the dataset \ncategories = ['Memorability', 'Learnability', 'Efficiency', 'Errors/Effectiveness', 'Satisfaction', \n\t\t\t'Aesthetics and Appeal',\n\t\t\t'Affect and Emotion',\n\t\t\t'Anticipation',\n\t\t\t'Comfort',\n\t\t\t'Detailed Usability',\n\t\t\t'Enchantment',\n\t\t\t'Engagement',\n\t\t\t'Enjoyment and Fun',\n\t\t\t'Frustration',\n\t\t\t'Hedonic',\n\t\t\t'Impact',\n\t\t\t'Likeability', \n\t\t\t'Motivation',\n\t\t\t'Overall Usability',\n\t\t\t'Pleasure',\n\t\t\t'Support',\n\t\t\t'Trust',\n\t\t\t'User Differences',\n\t\t\t'Bodily image and Appearance',\n\t\t\t'Concentration',\n\t\t\t'Energy',\n\t\t\t'Fatigue',\n\t\t\t'Learning',\n\t\t\t'Memory',\n\t\t\t'Negative feelings',\n\t\t\t'Pain and Discomfort', \n\t\t\t'Personal relationships',\n\t\t\t'Positive feelings',\n\t\t\t'Self-esteem',\n\t\t\t#'Sexual activity',\n\t\t\t'Sleep and Rest',\n\t\t\t'Social support',\n\t\t\t'Thinking']","metadata":{"id":"9Go25x_RX9fN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **DATA PRE-PROCESSING** \n\n\n","metadata":{"id":"ixLWGF_gfzgK"}},{"cell_type":"code","source":"import nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nimport re\n\nimport emoji\nimport unicodedata\nfrom nltk.corpus import wordnet\nfrom emoji.unicode_codes import UNICODE_EMOJI\nfrom textblob import TextBlob, Word\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\n\n\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nnltk.download('stopwords')\nnltk.download('omw')","metadata":{"id":"SxJCdegzgfZC","outputId":"44cfef15-0b0b-4eab-c7d5-6fd760eab76f","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def emojiToCLDRshortName(text):\n\ttry:\n\t\thas_emoji = bool(emoji.get_emoji_regexp().search(text))\n\t\tif (has_emoji):\n\t\t\temoji_chars = emoji.EMOJI_ALIAS_UNICODE_ENGLISH.values()\n\t\t\tdef _emoji(char):\n\t\t\t\tif char in emoji_chars:\n\t\t\t\t\treturn unicodedata.name(char) + \" \"\n\t\t\treturn ''.join(_emoji(char) or char for char in text)\n\t\telse:\n\t\t\treturn text\n\texcept Exception as e:\n\t\tprint(\"Emoji convert - \", e)\n\n\ndef contractions(text):\n\t# https://gist.github.com/nealrs/96342d8231b75cf4bb82\n\t# https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions\n\n\tcDict = {\n\t  \"ain't\": \"am not\",\n\t  \"aren't\": \"are not\",\n\t  \"can't\": \"cannot\",\n\t  \"can't've\": \"cannot have\",\n\t  \"'cause\": \"because\",\n\t  \"could've\": \"could have\",\n\t  \"couldn't\": \"could not\",\n\t  \"couldn't've\": \"could not have\",\n\t  \"didn't\": \"did not\",\n\t  \"doesn't\": \"does not\",\n\t  \"don't\": \"do not\",\n\t  \"hadn't\": \"had not\",\n\t  \"hadn't've\": \"had not have\",\n\t  \"hasn't\": \"has not\",\n\t  \"haven't\": \"have not\",\n\t  \"he'd\": \"he would\",\n\t  \"he'd've\": \"he would have\",\n\t  \"he'll\": \"he will\",\n\t  \"he'll've\": \"he will have\",\n\t  \"he's\": \"he is\",\n\t  \"how'd\": \"how did\",\n\t  \"how'd'y\": \"how do you\",\n\t  \"how'll\": \"how will\",\n\t  \"how's\": \"how is\",\n\t  \"I'd\": \"I would\",\n\t  \"I'd've\": \"I would have\",\n\t  \"I'll\": \"I will\",\n\t  \"I'll've\": \"I will have\",\n\t  \"I'm\": \"I am\",\n\t  \"I've\": \"I have\",\n\t  \"isn't\": \"is not\",\n\t  \"it'd\": \"it had\",\n\t  \"it'd've\": \"it would have\",\n\t  \"it'll\": \"it will\",\n\t  \"it'll've\": \"it will have\",\n\t  \"it's\": \"it is\",\n\t  \"let's\": \"let us\",\n\t  \"ma'am\": \"madam\",\n\t  \"mayn't\": \"may not\",\n\t  \"might've\": \"might have\",\n\t  \"mightn't\": \"might not\",\n\t  \"mightn't've\": \"might not have\",\n\t  \"must've\": \"must have\",\n\t  \"mustn't\": \"must not\",\n\t  \"mustn't've\": \"must not have\",\n\t  \"needn't\": \"need not\",\n\t  \"needn't've\": \"need not have\",\n\t  \"o'clock\": \"of the clock\",\n\t  \"oughtn't\": \"ought not\",\n\t  \"oughtn't've\": \"ought not have\",\n\t  \"shan't\": \"shall not\",\n\t  \"sha'n't\": \"shall not\",\n\t  \"shan't've\": \"shall not have\",\n\t  \"she'd\": \"she would\",\n\t  \"she'd've\": \"she would have\",\n\t  \"she'll\": \"she will\",\n\t  \"she'll've\": \"she will have\",\n\t  \"she's\": \"she is\",\n\t  \"should've\": \"should have\",\n\t  \"shouldn't\": \"should not\",\n\t  \"shouldn't've\": \"should not have\",\n\t  \"so've\": \"so have\",\n\t  \"so's\": \"so is\",\n    \"im\":\"i am\",\n\t  \"that'd\": \"that would\",\n\t  \"that'd've\": \"that would have\",\n\t  \"that's\": \"that is\",\n\t  \"thats\": \"that is\",\n\t  \"there'd\": \"there had\",\n\t  \"there'd've\": \"there would have\",\n\t  \"there's\": \"there is\",\n\t  \"they'd\": \"they would\",\n\t  \"they'd've\": \"they would have\",\n\t  \"they'll\": \"they will\",\n\t  \"they'll've\": \"they will have\",\n\t  \"they're\": \"they are\",\n\t  \"they've\": \"they have\",\n\t  \"to've\": \"to have\",\n\t  \"wasn't\": \"was not\",\n\t  \"we'd\": \"we had\",\n\t  \"we'd've\": \"we would have\",\n\t  \"we'll\": \"we will\",\n\t  \"we'll've\": \"we will have\",\n\t  \"we're\": \"we are\",\n\t  \"we've\": \"we have\",\n\t  \"weren't\": \"were not\",\n\t  \"what'll\": \"what will\",\n\t  \"what'll've\": \"what will have\",\n\t  \"what're\": \"what are\",\n\t  \"what's\": \"what is\",\n\t  \"what've\": \"what have\",\n\t  \"when's\": \"when is\",\n\t  \"when've\": \"when have\",\n\t  \"where'd\": \"where did\",\n\t  \"where's\": \"where is\",\n\t  \"where've\": \"where have\",\n\t  \"who'll\": \"who will\",\n\t  \"who'll've\": \"who will have\",\n\t  \"who's\": \"who is\",\n\t  \"who've\": \"who have\",\n\t  \"why's\": \"why is\",\n\t  \"why've\": \"why have\",\n\t  \"will've\": \"will have\",\n\t  \"won't\": \"will not\",\n\t  \"won't've\": \"will not have\",\n\t  \"would've\": \"would have\",\n\t  \"wouldn't\": \"would not\",\n\t  \"wouldn't've\": \"would not have\",\n\t  \"y'all\": \"you all\",\n\t  \"y'alls\": \"you alls\",\n\t  \"y'all'd\": \"you all would\",\n\t  \"y'all'd've\": \"you all would have\",\n\t  \"y'all're\": \"you all are\",\n\t  \"y'all've\": \"you all have\",\n\t  \"you'd\": \"you had\",\n\t  \"you'd've\": \"you would have\",\n\t  \"you'll\": \"you you will\",\n\t  \"you'll've\": \"you you will have\",\n\t  \"you're\": \"you are\",\n\t  \"you've\": \"you have\"\n\t}\n\ttry:\n\t\tc_re = re.compile('(%s)' % '|'.join(cDict.keys()))\n\n\t\tdef expandContractions(text, c_re=c_re):\n\t\t\tdef replace(match):\n\t\t\t\treturn cDict[match.group(0)]\n\t\t\treturn c_re.sub(replace, text)\n\t\t\n\t\ttext = expandContractions(text.lower())\n\t\treturn text\n\texcept Exception as e:\n\t\tprint(\"contractions\", e)\n\n\ndef repeatedCharacters(text):\n\t# https://www.nltk.org/_modules/nltk/tokenize/casual.html#reduce_lengthening\n\ttry:\n\t\tif (len(text) > 1):\n\t\t\tpattern = re.compile(r\"(.)\\1{2,}\") #regex\n\t\t\treturn pattern.sub(r\"\\1\\1\\1\", text)\n\t\telse:\n\t\t\treturn text\n\texcept Exception as e:\n\t\tprint(\"repeatedCharacters error - \", e)\n\n\ndef spellCorrection(text):\n\ttry:\n\t\ttext = TextBlob(text).correct()\n\texcept Exception as e:\n\t\tprint(\"spellCorrection error - \", e)\n  \n\treturn text\n\ndef slangs(text):\n  try:\n    file = 'https://raw.githubusercontent.com/renatojmsantos/An-Opinion-Mining-Approach-to-Analyse-Perceived-Impacts-of-Games-for-Health/main/acronimos.csv'\n    acron = pd.read_csv(file,lineterminator='\\n',encoding='utf-8')\n\n    slang = acron['slang']\n    complete = acron['complete']\n    row = 0\n    for s in slang:\n      if(slang[row] in text.lower().strip().split()):\n        text = text.lower()\n        text = text.replace(slang[row],str(complete[row].strip()))\n      else:\n        row+=1\n    return text\n  except IOError as e:\n    print(e)\n  except FileNotFoundError:\n    print(\"File not found...\", file)\n  except Exception as ex:\n    print(\"slangs...\", ex)\n  \n  \ndef clearText(text):\n  try:\n    # contractions\n    #text = contractions(str(text))\n    \n    #acronyms\n    #text = slangs(str(text))\n\n    # emojis to cldr\n    text = str(emojiToCLDRshortName(str(text)))\n    # remove URLs\n    text = re.sub('https?://[A-Za-z0-9./?&=_]+','',text)\n    # hashtags\n    text = re.sub('#[A-Za-z0-9]+','',text)\n    # mentions\n    text = re.sub('@[A-Za-z0-9._-]+','',text)\n    # to lower\n    text = text.lower()\n    # remove pontuation\n    text = re.sub(r\"[^\\w\\s]\",\"\",text)\n    #remove white spaces\n    text = \" \".join(text.strip().split())\n    text = re.sub(r\"[\\W\\s]\",\" \",text)\n    text = re.sub(\"\\n\",\"\",text)\n    # replace just dance with game... important to consider dance as activity\n    text = re.sub(\"just dance\",\"game\",text)\n    #remove repeted characters\n    text = repeatedCharacters(text)\n    #do some spell correction\n    #text = spellCorrection(text) \n  except Exception as e:\n    print(\"clearText error - \", e)\n\n  return text\n\ndef pos_tagger(nltk_tag):\n  if nltk_tag.startswith('J'):\n\t\t\treturn wordnet.ADJ\n  elif nltk_tag.startswith('V'):\n    return wordnet.VERB\n  elif nltk_tag.startswith('N'):\n    return wordnet.NOUN\n  #elif nltk_tag.startswith('R'):\n  #  return wordnet.ADV\n  else:          \n    return None\n\ndef stemmingText(text):\n  #print(text)\n  # get adj, verb, nouns\n  textWords = word_tokenize(text)\n  pos_tagged = nltk.pos_tag(textWords)\n  \n  wordnet_tagged = list(map(lambda x: (x[0], pos_tagger(x[1])), pos_tagged))\n\n  #print(wordnet_tagged)\n  # stemming\n  stemmer = nltk.stem.SnowballStemmer('english') \n  stemSentence = \"\"\n  for word, tag in wordnet_tagged:\n    if (tag is not None):\n      stem = stemmer.stem(word)\n      stemSentence+=stem\n      stemSentence += \" \"\n  stemSentence = stemSentence.strip()\n  #print(stemSentence)\n  \n  # remove stop words\n  words = word_tokenize(stemSentence)\n  stopwords = nltk.corpus.stopwords.words('english')\n  pals = [word for word in words if not word in stopwords] \n  text = \" \".join(pals)\n  return text\n","metadata":{"id":"IA9SpwsLJ85Z","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef runPreprocessing(text):\n  text = clearText(text)\n  text = stemmingText(str(text))\n  return text\n\n#example = \"I love playing just dance with my friends, OMG It is VERYYY cool !!!! 😂 😂 😂😂😂\"\n#print(runPreprocessing(str(example)))\n\n#data['originalText'] = data['originalText'].apply(runPreprocessing)\n#data.head()\n","metadata":{"id":"i4JGmOHgKFJK","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"\n# TF-IDF VECTORIZER\n\n","metadata":{"id":"QXyAaJSQ_Hkr"}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ntrain, test = train_test_split(data, random_state=20, test_size=0.20) #, shuffle=True\n#print(train.shape, test.shape)\n#print(train[categories].sum())\ntrain_text = train['originalText']\ntest_text = test['originalText']\n\n#vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\nvectorizer = TfidfVectorizer(preprocessor=runPreprocessing)\n\nx_train = vectorizer.fit_transform(train_text)\nx_test = vectorizer.transform(test_text)\n\n#vectorizer.get_feature_names_out()","metadata":{"id":"ZKe0paZEWPnz","outputId":"0fa48c64-3b68-4a98-e0dc-3a0ef4ff3002","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import Packages","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.multiclass import OneVsRestClassifier\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.svm import LinearSVC\n\nimport imblearn\nfrom imblearn.over_sampling import SMOTE","metadata":{"id":"U_jFzDxlhoqP","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model Classification & Handling with inbalance dataset & Predict new text","metadata":{"id":"_XNuozbP_cZ0"}},{"cell_type":"code","source":"Model = OneVsRestClassifier(LinearSVC())\n#Model = OneVsRestClassifier(SVC(kernel = 'linear',C=0.01,gamma='auto'))\n#Model = OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=-1)\n\nprint(\"{:<30} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format('Topic','Accuracy','Recall','Precision','F-1 Score', 'ROC_AUC', 'Balanced'))\n\n#new_text = \"I love play just dance with friends\"\nnew_text = \"THIS GAME IS HORRIBLE!\"\n#new_text = \"I am thinking about playing this with my friends and family. It is so FUN!\"\n\n#clean_text = runPreprocessing(new_text)\ntext_pred = vectorizer.transform([new_text])\ntopics_predicted = []\n\nfor category in categories:\n    x = x_train\n    y = train[category]\n    \n    #balance dataset with Synthetic minority oversampling\n    smote = SMOTE()\n    x_smote, y_smote = smote.fit_resample(x, y)\n    \n    Model.fit(x_smote, y_smote)\n\n    y_pred = Model.predict(x_test)\n    y_true = test[category]\n\n    accuracy = \"{:.3f}\".format(accuracy_score(y_true, y_pred))\n    recall = \"{:.3f}\".format(recall_score(y_true, y_pred, average='weighted'))\n    precision = \"{:.3f}\".format(precision_score(y_true, y_pred, average='micro'))\n    f1 = \"{:.3f}\".format(f1_score(y_true, y_pred, average='weighted'))\n    roc_auc = \"{:.3f}\".format(roc_auc_score(y_true, y_pred))\n    balanced = \"{:.3f}\".format(balanced_accuracy_score(y_true, y_pred))\n\n    print(\"{:<30} {:<10} {:<10} {:<10} {:<10} {:<10} {:<10}\".format(str(category), accuracy, recall, precision, f1, roc_auc, balanced))\n    #print(confusion_matrix(y_pred, y_true))\n    \n    #text_pred = vectorizer.transform([clean_text])\n    if([1] in (Model.predict(text_pred).astype(int))):\n      topics_predicted.append(category)\n\nprint(\"\\nNEW TEXT: '\",new_text,\"'\\nPREDICTED: \",topics_predicted)\n","metadata":{"id":"BYyUBaQThrn7","outputId":"d04eb122-7722-41c1-c0d8-1d26336732a2","trusted":true},"execution_count":null,"outputs":[]}]}